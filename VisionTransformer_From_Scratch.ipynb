{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlde-ms/vision-transformer-from-scratch.git\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import mnist_vit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisionTransformer from Scratch\n",
    "\n",
    "This notebook demonstrates how the popular VisionTransformer (ViT) architecture, presented in the paper TODO, works. It is also implemented in [`torchvision`](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py), which this notebook uses as inspiration.\n",
    "\n",
    "First, we explore the modules used in the ViT, successively building larger of its parts into a minimal working version. While doing so, we explore the *code*, *graphics* representing the architecture, understanding the *math* behind it and looking at the transformations the *tensor* flowing through the network undergoes to produce the output. Second, we define a very tiny ViT to solve the toy *example* of [MNIST](https://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The General MLP Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source Code](https://github.com/pytorch/vision/blob/main/torchvision/ops/misc.py)\n",
    "\n",
    "<img src=\"TODO\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    \"\"\"Multi-layer perceptron (MLP) module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = nn.ReLU,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        params = {}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim  # update input dimension for next layer\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer MLP Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer MLP Block is a special case of the General MLP Module. There are always 2 hidden layers with a GELU activation function and special initialization of the weights. Usually `mlp_dim` = 4 $\\cdot$ `in_dim`.\n",
    "\n",
    "<img src=\"TODO\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"TODO\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, return_attention_output=False):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "\n",
    "        # Added to get attention output\n",
    "        if return_attention_output: return x\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"TODO\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the follwing functionality I have added to make to the model easier to probe:\n",
    "\n",
    "1. Added some `print` statements printing the shape of the tensor at the current stage, which can be turned on and off via the `print_shapes` parameter of the constructor.\n",
    "2. Added the method `get_attention_head` to be able to inspect the output of a given MultiheadAttention module.\n",
    "3. Added the `in_channels` parameter to the constructor to be able to specify the number of channels (was hardcoded to 3, but we have only 1 with MNIST)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"TODO\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        print_shapes: bool = False,  # Added by me to print the shapes of the tensors\n",
    "        in_channels: int = 3,        # Added by me to specify the number of input channels (needed, as MNIST has only 1 channel)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "        self.print_shapes = print_shapes  # Added by me to print the shapes of the tensors\n",
    "\n",
    "        # Patchify and flatten input\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        # Number of patches\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        # self.class_token = nn.Parameter(torch.rand(1, 1, hidden_dim))\n",
    "        seq_length += 1\n",
    "\n",
    "        # Creating the encoder\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Projection head\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        # Init the patchify stem\n",
    "        fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "        nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "        if self.conv_proj.bias is not None:\n",
    "            nn.init.zeros_(self.conv_proj.bias)\n",
    "\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        if (self.print_shapes): print(\"After Projection:\", x.shape)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "        if (self.print_shapes): print(\"After Reshape:\", x.shape)\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if (self.print_shapes): print(\"After Permute:\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "    # Added to be able to inspect the output of MultiheadAttention modules\n",
    "    def get_attention_output(self, x: torch.Tensor, layer: int, head: int) -> torch.Tensor:\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "        for i, encoder_layer in enumerate(self.encoder.layers):\n",
    "            if i == layer:\n",
    "                x = encoder_layer(x, return_attention_output=True)\n",
    "                break\n",
    "            else:\n",
    "                x = encoder_layer(x)\n",
    "        head_dim = self.hidden_dim // self.encoder.layers[layer].num_heads\n",
    "        x = x.view(n, -1, self.encoder.layers[layer].num_heads, head_dim)\n",
    "        x = x[:, :, head, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if (self.print_shapes): print(\"Input:\", x.shape)\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "        if (self.print_shapes): print(\"Batch Size:\", n)\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "        if (self.print_shapes): print(\"Batch Class Token:\", batch_class_token.shape)\n",
    "        if (self.print_shapes): print(\"Encoder Input:\", x.shape)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        if (self.print_shapes): print(\"Encoder Output:\", x.shape)\n",
    "        \n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "        if (self.print_shapes): print(\"Only Batch Class Token:\", x.shape)\n",
    "\n",
    "        x = self.heads(x)\n",
    "        if (self.print_shapes): print(\"Output:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Vision Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how the Vision Transformer is assembled bottom-up, we can have a look at the whole architecture, see how the tensors flow through it, and pass inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShapes of the tensor at different stages:\u001b[0m\n",
      "Input: torch.Size([1, 3, 512, 512])\n",
      "After Projection: torch.Size([1, 768, 32, 32])\n",
      "After Reshape: torch.Size([1, 768, 1024])\n",
      "After Permute: torch.Size([1, 1024, 768])\n",
      "Batch Size: 1\n",
      "Batch Class Token: torch.Size([1, 1, 768])\n",
      "Encoder Input: torch.Size([1, 1025, 768])\n",
      "Encoder Output: torch.Size([1, 1025, 768])\n",
      "Only Batch Class Token: torch.Size([1, 768])\n",
      "Output: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(512, 16, 6, 6, 768, 3072, num_classes=1000, representation_size=2000, print_shapes=True)\n",
    "\n",
    "image = Image.open(\"windmill.png\")\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0), (1))\n",
    "])\n",
    "# Normalize the image\n",
    "image = normalize(image)\n",
    "# Add a batch dimension of 1\n",
    "image = image[None]\n",
    "\n",
    "# Run the image through the model and print the shapes at different stages\n",
    "print(\"\\033[1mShapes of the tensor at different stages:\\033[0m\")\n",
    "out = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [1, 1000]                 768\n",
       "├─Conv2d: 1-1                                 [1, 768, 32, 32]          590,592\n",
       "├─Encoder: 1-2                                [1, 1025, 768]            787,200\n",
       "│    └─Dropout: 2-1                           [1, 1025, 768]            --\n",
       "│    └─Sequential: 2-2                        [1, 1025, 768]            --\n",
       "│    │    └─EncoderBlock: 3-1                 [1, 1025, 768]            7,087,872\n",
       "│    │    └─EncoderBlock: 3-2                 [1, 1025, 768]            7,087,872\n",
       "│    │    └─EncoderBlock: 3-3                 [1, 1025, 768]            7,087,872\n",
       "│    │    └─EncoderBlock: 3-4                 [1, 1025, 768]            7,087,872\n",
       "│    │    └─EncoderBlock: 3-5                 [1, 1025, 768]            7,087,872\n",
       "│    │    └─EncoderBlock: 3-6                 [1, 1025, 768]            7,087,872\n",
       "│    └─LayerNorm: 2-3                         [1, 1025, 768]            1,536\n",
       "├─Sequential: 1-3                             [1, 1000]                 --\n",
       "│    └─Linear: 2-4                            [1, 2000]                 1,538,000\n",
       "│    └─Tanh: 2-5                              [1, 2000]                 --\n",
       "│    └─Linear: 2-6                            [1, 1000]                 2,001,000\n",
       "===============================================================================================\n",
       "Total params: 47,446,328\n",
       "Trainable params: 47,446,328\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 636.66\n",
       "===============================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 277.11\n",
       "Params size (MB): 129.94\n",
       "Estimated Total Size (MB): 410.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model summary\n",
    "summary(VisionTransformer(512, 16, 6, 6, 768, 3072, num_classes=1000, representation_size=2000), input_size=(1, 3, 512, 512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Our ViT to an Example: MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(28, 2, 2, 2, 64, 256, num_classes=10, representation_size=32, in_channels=1, print_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for evaluation.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "6.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Accuracy on the train data: 95.88%\n",
      "[[ 957    0    3    0    2    2    6    0    5    2]\n",
      " [   0 1121    0    3    0    0    2    4    2    1]\n",
      " [   6    2  984    5    1    0    0   14    5    3]\n",
      " [   2    1   15  966    0   15    0    7   41    2]\n",
      " [   1    3    4    2  943    1    5   11    8   21]\n",
      " [   1    0    0   19    0  858    5    0   17    3]\n",
      " [   7    4    7    0    8    5  938    0    5    2]\n",
      " [   0    1   14   11    1    3    0  976    5    8]\n",
      " [   3    2    5    3    1    2    2    1  883    5]\n",
      " [   3    1    0    1   26    6    0   15    3  962]]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to also train the model\n",
    "# mnist_vit.train(model)\n",
    "\n",
    "# Expects model weights to be stored in `vit_weights.pth`\n",
    "mnist_vit.evaluate(model)\n",
    "\n",
    "# Expects model weights to be stored in `vit_weights.pth`\n",
    "mnist_vit.visualize_attention(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can look at the images generated in the folder `attention` and inspect the attention heads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
